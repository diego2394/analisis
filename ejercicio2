#ejercico clase 


y <- c(25.5, 31.2, 25.9, 38.4, 18.4, 26.7, 26.4,25.9,32,25.2,39.7,35.7,26.5)
x1 <- c(1.74,6.32,6.2,10.52,1.19,1.22,4.10,6.32,4.08,4.15,10.15,1.72,1.70)
x2 <- c(5.30,5.42,8.41,4.63,11.60,5.85,6.62,8.72,4.42,7.60,4.83,3.12,5.3)
x3 <- c(10.8,9.4,7.2,8.5,9.4,9.9,8,9.1,8.7,9.2,9.4,7.6,8.2)
 ##0) analizar graficos de dispersion y pruebas de correlacion
## 1) plantear la ecuacion estimada origina, es decir con todas las variables
## 2) analizar la anova, r cuadrada, estadistico F (mediante p.h.)
# 3) comparen modelos con backward y analicen el modelo que genere la funcion step
#realizar la prueba de tres supuestos a este modelo con (kolmogro, lewandouski, smirnov, breuch- pagan,
#rubin watson) analise los resultados con estas prubas
z <- data.frame(y,x1,x2,x3)
 
cor(z, use ="everything", method = "pearson")
M<-lm(y~x1+x2+x3)
summary(M)
##El modelo es aceptable ya que el p- value es menor a .05 por lo que se acepta y nuestro 
#f supera el valor de 1 (31.04)

plot (z, pch = 19, main = "grafica de dispercion", col="darkgreen")
#la ecuacion que no arrojan los resultados es
y = 39.1767 + 1.0166*x1 - 1.8608*x2 - 0.3461*x3

anova(M)
#mustra que la varible x3 no tine relacion con la varibale independiente por lo que tmamos la decision
#de elminiar esta variale
step(M, direction = "backward")
z1 <- data.frame(y,x1,x2)
z1 <- z[,-4]

m<-lm(y~x1+x2)
summary(m)
#al eliminar la variable se vulve mas estadisticamente significativo ya que nuestro p- value baja y 
#nuestra r cuadrad llega casi a 1

m$ajustados <- fitted(m)#obtiene los valores ajustados del modelo, Y gorrito
m$res <- residuals(m)#se espera que esten cercanos a cero
m$rstud <- rstudent(m)#RESIDUALES/ ERRORES
install.packages("lmtest")
require(lmtest)
ks.test(m$rstud, "pnorm")

bptest(m, studentize = FALSE, data = m)

dwtest(m, alternative = "two.sided", data=z1)
